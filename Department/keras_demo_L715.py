# -*- coding: utf-8 -*-
"""keras-demo-l715.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fDBelzUjGFUJFFCUGDfljat5g7BYTAcS

# Keras demo for L715

## Import the library

* Import **TensorFlow**
 * Wait... What is **TensorFlow**?
  * TensorFlow is an open-source machine learning library for research and production. 
  * TensorFlow offers APIs for beginners and experts to develop for desktop, mobile, web, and cloud.
  * Useful links:
   * https://www.tensorflow.org/
   * https://www.tensorflow.org/tutorials/representation/word2vec
"""

import tensorflow as tf

"""* Import **Keras**
 * What? I thought we are talking about TensorFlow?
 * The high-level Keras API provides building blocks to create and train deep learning models. (Like LEGOs?)
 * Keras is much easier to start with than plain TensorFlow, but if you want to do something with Keras that doesn't come out of the box, it will be harder to implement that.
 * TensorFlow, on the other hand, allows you to create any arbitrary computational graph, providing much more flexibility. If you are doing more research work, TensorFlow is the route to go due to this flexibility.
"""

from tensorflow import keras

"""## Load the dataset

* Download the IMDB dataset
 * Let's see how Keras performs on the same dataset compared to scikit-learn. :)
"""

imdb = keras.datasets.imdb

(train_data, train_labels),(test_data, test_labels) = imdb.load_data(num_words=10000)

"""* Stop right there!
 * How do you know the dataset is there?
   * Typically these kind of learning libraries come with several datasets for people to get started or play with.
 * And... how do you know you need to write a code block like that to get the dataset?
   * https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification

## Explore the dataset

* OK... but how do the data look like?
"""

print("Training entries: {}, labels: {}".format(len(train_data),len(train_labels)))

print(train_data[0])

"""* What are those numbers?
 * x_train, x_test: list of sequences, which are lists of indices (integers).
 * So... exactly what are those?
  * This is only for curiosity purposes... In fact, you almost always will have to prepare the data yourself.
"""

################################################################################

# Convert the integers back to words
word_index = imdb.get_word_index()

# The first indices are reserved
word_index = {k:(v+3) for k,v in word_index.items()}
word_index["<PAD>"] = 0
word_index["<START>"] = 1
word_index["<UNK>"] = 2  # unknown
word_index["<UNUSED>"] = 3

reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

def decode_review(text):
    return ' '.join([reverse_word_index.get(i, '?') for i in text])

################################################################################

print(decode_review(train_data[0]))

"""* We'll discuss this block of code if we have time. For now, let's move on."""

print(len(train_data[0]),len(train_data[1]))

"""## Preparing the data

* They are of different lengths. What do we do?
"""

train_data = keras.preprocessing.sequence.pad_sequences(train_data,
                                                        value=word_index["<PAD>"],
                                                        padding='post',
                                                        maxlen=256)

test_data = keras.preprocessing.sequence.pad_sequences(test_data,
                                                       value=word_index["<PAD>"],
                                                       padding='post',
                                                       maxlen=256)

"""* What's that?
  * **Padding**. :)
* How do you know this padding technique?
  * Read papers and play with code other people wrote.
* How do you know the **arguments**?
  * https://keras.io/preprocessing/sequence/
* How do you know the **values** for the arguments?
 * That's pretty empirical... You'll have to try (or see how others do it).
* So... how do they look now?
"""

print(len(train_data[0]))

print(len(train_data[1]))

print(train_data[0])

print(decode_review(train_data[0]))

"""## Build the model

* Let's build a model! (Yes, finally... Data cleaning is tedious work...)
* OK... Where do we start?
  * Honestly? Let's steal somebody else's model and try first.
 
* Let's start with something really simple -- a sequential model.
  * All layers are just stacked. Nothing fancy.
  * https://keras.io/getting-started/sequential-model-guide/
"""

model = keras.Sequential()

"""* The first layer is an Embedding layer. 
 * This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. 
  * These vectors are learned as the model trains. 
  * The vectors add a dimension to the output array. 
  * The resulting dimensions are: (batch, sequence, embedding).
  * https://keras.io/layers/embeddings/
"""

vocab_size = 10000

model.add(keras.layers.Embedding(vocab_size, 16))

"""* A GlobalAveragePooling1D layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.
  * https://keras.io/layers/pooling/
"""

model.add(keras.layers.GlobalAveragePooling1D())

"""* This fixed-length output vector is piped through a fully-connected layer with 16 hidden units.
  * https://keras.io/layers/core/
"""

model.add(keras.layers.Dense(16, activation=tf.nn.relu))

"""* The last layer is densely connected with a single output node. 
* Using the sigmoid activation function, this value is a float between 0 and 1, representing a probability, or confidence level.
  * Why? --> [Common activation functions](https://qph.fs.quoracdn.net/main-qimg-d131b1b1ffb1ae9d842e135f05635f1c)
"""

model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))

"""## Review the model

* Let's see how our model looks.
"""

model.summary()

"""## Optimize the model

* So... We have a model now. 
 * How do we find out the best set of parameters?
    * Optimize using a loss function.
      * Optimizers: https://keras.io/optimizers/
        * [Haddling the saddle point](https://i.stack.imgur.com/gjDzm.gif)
      * Loss function:
        * Losses: https://keras.io/losses/
"""

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

"""## Create a validation set

* We need a validation set to see how well we do.
"""

x_val = train_data[:10000]

partial_x_train = train_data[10000:]

y_val = train_labels[:10000]

partial_y_train = train_labels[10000:]

"""## Let's train the model!"""

history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=40,
                    batch_size=512,
                    validation_data=(x_val, y_val),
                    verbose=1)

"""## Evaluate the model

* So... Exactly how well did we do?
"""

results = model.evaluate(test_data, test_labels)
print(results)

"""## How do you know...?

* What is the best architecture and hyperparameters?
  * That's like **the ultimate** research question. 
  * Something a bit more intelligient:
    * [**NOT** Everything you need to know about AutoML and Neural Architecture Search](https://towardsdatascience.com/everything-you-need-to-know-about-automl-and-neural-architecture-search-8db1863682bf)
    * [AdaNet](https://github.com/tensorflow/adanet)
    * [AutoKeras](https://autokeras.com/)

## Visualize it!
"""

history_dict = history.history
history_dict.keys()

import matplotlib.pyplot as plt

acc = history_dict['acc']
val_acc = history_dict['val_acc']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

plt.clf()   # clear figure

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()